{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5101f5a2",
   "metadata": {},
   "source": [
    "# Contextual Bayesian Optimisation with Large Language Models via In-Context-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d590f50",
   "metadata": {},
   "source": [
    "This notebook will execute Contextual Bayesian Optimisation, in attempt to detect whether LLMs can learn contextual information.\n",
    "\n",
    "**NOTE:** Before running this file, you must fix the relative import issues and also add the src to the PYTHONPATH. Additionally, you may need to pass in parameters to save files etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589ed6b",
   "metadata": {},
   "source": [
    "<DIV STYLE=\"background-color:#000000; height:10px; width:100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b774b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe06c92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T13:57:28.705339522Z",
     "start_time": "2023-07-13T13:57:28.658785593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "# Third Party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Private\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "from src.main import run_bo_vs_c_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e27a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T12:13:56.759964371Z",
     "start_time": "2023-07-13T12:13:56.749792570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b067b4",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf92c9",
   "metadata": {},
   "source": [
    "The dataset can be found here [BigSOL](https://zenodo.org/record/6809669). We use this for contextual BO as we are provided with varying context, for the same compound and solute. In the original BO setup of BO-LIFT, we want to remove points because there is no context and the goal is to obtain the values of as many new points as possible to find the maximum. In the contextual BO setup, the goal is different - we want to choose the appropriate point for the current context, which can be anything from the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, num_occurrences_low, num_occurrences_high, temps, num_smiles):\n",
    "    data = data.dropna()\n",
    "    data = data.drop_duplicates().reset_index(drop=True)\n",
    "    data.rename(columns={\"T,K\": \"Temperature\"}, inplace=True)\n",
    "    data = data.sort_values(by=\"SMILES\")\n",
    "    # Shrink dataset\n",
    "    main_data = pd.DataFrame(columns=[\"SMILES\"] + list(data[\"Temperature\"].unique()))\n",
    "    for smile in data[\"SMILES\"].unique():\n",
    "        sub_result = data[data[\"SMILES\"] == smile]\n",
    "        sub_temp = {\"SMILES\": smile}\n",
    "        sub_temp.update(dict(sub_result[\"Temperature\"].value_counts()))\n",
    "        for temp in list(main_data.columns):\n",
    "            if temp not in sub_temp.keys():\n",
    "                sub_temp[temp] = 0\n",
    "        main_data = pd.concat(\n",
    "            (pd.DataFrame([sub_temp], columns=list(main_data.columns)), main_data)\n",
    "        )\n",
    "    sub_data = main_data[[\"SMILES\"] + temps]\n",
    "    mask = (sub_data.iloc[:, 1:] > num_occurrences_low) & (\n",
    "        sub_data.iloc[:, 1:] < num_occurrences_high\n",
    "    )\n",
    "    mask = mask.all(axis=1)\n",
    "    refined_data = sub_data[mask]\n",
    "    refined_data = refined_data[refined_data.iloc[:, 1:].ge(3).all(axis=1)]\n",
    "    refined_data = refined_data[refined_data[\"SMILES\"].apply(lambda x: len(x) < 30)][\n",
    "        :num_smiles\n",
    "    ]\n",
    "    combined_data = data.merge(refined_data[\"SMILES\"], on=\"SMILES\")\n",
    "    combined_data = combined_data[combined_data[\"Temperature\"].isin(temps)]\n",
    "    # Final dataframe\n",
    "    combined_data.rename(columns={\"SMILES_Solvent\": \"SMILES Solvent\"}, inplace=True)\n",
    "    combined_df = combined_data[\n",
    "        [\"SMILES\", \"Temperature\", \"SMILES Solvent\", \"Solubility\"]\n",
    "    ].reset_index(drop=True)\n",
    "    return combined_df, refined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicate pairs of SMILES and SMILES Solvents\n",
    "df_new = pd.read_csv(\"src\\data\\bigsoldb.csv\")\n",
    "duplicates = df_new[df_new.duplicated(subset=['SMILES', 'SMILES_Solvent'], keep=False)]\n",
    "bigsoldb_df, bigsoldmask_df = create_dataset(\n",
    "    data=duplicates,\n",
    "    num_occurrences_low=5,\n",
    "    num_occurrences_high=20,\n",
    "    temps=[313.15, 308.15, 303.15, 298.15],\n",
    "    num_smiles=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f42b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "df = pd.DataFrame(bigsoldb_df)\n",
    "# Group by SMILES and SMILES Solvent, and count the number of occurrences\n",
    "grouped = df.groupby(['SMILES', 'SMILES Solvent']).size().reset_index(name='count')\n",
    "# Filter the rows where the count is even\n",
    "even_rows = grouped[grouped['count'] == 4]\n",
    "# Merge the filtered rows back to the original dataframe\n",
    "filtered_df = pd.merge(df, even_rows[['SMILES', 'SMILES Solvent']], on=['SMILES', 'SMILES Solvent'])\n",
    "while True:\n",
    "    # Find the pairs that maximize solubility at each temperature\n",
    "    max_solubility_pairs = filtered_df.groupby('Temperature').apply(lambda x: x.loc[x['Solubility'].idxmax()])[['SMILES', 'SMILES Solvent']]\n",
    "    # Check if at least two of those pairs differ\n",
    "    if len(max_solubility_pairs.drop_duplicates()) > 1:\n",
    "        break\n",
    "    # Remove the instances of the pairs that maximize solubility at each temperature\n",
    "    for _, row in max_solubility_pairs.iterrows():\n",
    "        filtered_df = filtered_df[(filtered_df['SMILES'] != row['SMILES']) | (filtered_df['SMILES Solvent'] != row['SMILES Solvent'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a75443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial modification\n",
    "final_df = filtered_df.reset_index(drop=True)\n",
    "final_df = pd.merge(final_df,final_df.groupby(['SMILES', 'SMILES Solvent']).size().reset_index(name='count').sample(169)[75:100].drop(\"count\", axis=1))\n",
    "final_df.to_csv(\"bo_vs_cbo_multi_context_100_4_temp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_df(df):\n",
    "    # Identify rows with maximum solubility at each temperature\n",
    "    max_row_30815 = df.loc[df[(df['Temperature'] == 308.15)]['Solubility'].idxmax()]\n",
    "    max_row_31315 = df.loc[df[(df['Temperature'] == 313.15)]['Solubility'].idxmax()]\n",
    "    # Check if the SMILES and SMILES Solvent pairs in these rows are the same\n",
    "    row1= pd.Series(max_row_30815[['SMILES', 'SMILES Solvent']])\n",
    "    row2 = pd.Series(max_row_31315[['SMILES', 'SMILES Solvent']])\n",
    "    counter = 1\n",
    "    while (row1['SMILES'] == row2['SMILES']) and (row1['SMILES Solvent'] == row2['SMILES Solvent']):\n",
    "        df = df.drop(max_row_31315.name)\n",
    "        df = df.drop(max_row_30815.name)\n",
    "        df = df[df.duplicated(subset=['SMILES', 'SMILES Solvent'], keep=False)].reset_index(drop=True)\n",
    "        # Identify rows with maximum solubility at each temperature\n",
    "        max_row_30815 = df.loc[df[(df['Temperature'] == 308.15)]['Solubility'].idxmax()]\n",
    "        max_row_31315 = df.loc[df[(df['Temperature'] == 313.15)]['Solubility'].idxmax()]\n",
    "        # Check if the SMILES and SMILES Solvent pairs in these rows are the same\n",
    "        row1= pd.Series(max_row_30815[['SMILES', 'SMILES Solvent']])\n",
    "        row2 = pd.Series(max_row_31315[['SMILES', 'SMILES Solvent']])\n",
    "        print(row1, row2)\n",
    "        counter += 1\n",
    "        if counter >= 4:\n",
    "            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3cf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate maximium value for each unique temperature\n",
    "def data_max(data):\n",
    "    max_temp = pd.DataFrame(columns=list(data.columns))\n",
    "    for i, temp in enumerate(data[\"Temperature\"].unique()):\n",
    "        sub_result = data[data[\"Temperature\"] == temp].reset_index(drop=True)\n",
    "        max_index = sub_result[\"Solubility\"].idxmax()\n",
    "        final_result = sub_result.loc[max_index]\n",
    "        max_temp.loc[i] = final_result\n",
    "    return max_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ecb49b",
   "metadata": {},
   "source": [
    "# ICL Contextual Bayesian Optimisation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6e349",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - Sometimes this function runs into API request issues - if so, run this on PyCharm through the test file in debug mode.\n",
    "kwargs = {\"data\": final_df,\n",
    "          \"N\": 50,\n",
    "          \"M\": 5,\n",
    "          \"num_train\": [5, 15], \n",
    "          \"models_list\": [\"gpt-3.5-turbo\"],\n",
    "          \"_lambda\": [1, 5, 10]}\n",
    "bo_vs_cbo_results = run_bo_vs_c_bo(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456d7eb",
   "metadata": {},
   "source": [
    "Now choose a file path to save the results and load this in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Use pickle to dump the results - you will have to supply this code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b94c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"...\"\n",
    "with open(file_path, \"rb\") as file:\n",
    "    loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22800ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results, selector, component):\n",
    "    df = pd.DataFrame(columns=[\"Strategy\", \"Method\", \"Selection\", f\"{component}\"])\n",
    "    i = 0\n",
    "    for method, method_data in results.items():\n",
    "        # Iterate through the second-level dictionary\n",
    "        for lift_type, lift_data in method_data.items():\n",
    "            # Iterate through the 'Optimal Point with MMR' and 'Optimal Point without MMR' entries\n",
    "            for j, (mmr_type, mmr_data) in enumerate(lift_data.items()):\n",
    "                component_values = [\n",
    "                    point[component] for point in results[method][lift_type][mmr_type]\n",
    "                ]\n",
    "                if len(component_values) != 0:\n",
    "                    df.loc[i] = {\n",
    "                        \"Strategy\": method,\n",
    "                        \"Method\": lift_type,\n",
    "                        \"Selection\": selector[i],\n",
    "                        f\"{component}\": component_values,\n",
    "                    }\n",
    "                    i += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3076ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain simplified results table\n",
    "results = loaded_data[\"upper_confidence_bound\"]\n",
    "strategies = [\"BO\", \"C-BO\"]\n",
    "methods = [\"BO-LLM\", \"CBO-LLM\"]\n",
    "selector = [\"with MMR\", \"with MMR\"]\n",
    "component = \"Regret\"\n",
    "full_df = pd.DataFrame(columns=[\"Strategy\", \"Method\", \"Selection\", f\"{component}\"])\n",
    "for result in results:\n",
    "    df = process_results(results=result, selector=selector, component=component)\n",
    "    full_df = pd.concat((df, full_df))\n",
    "full_df = full_df.reset_index(drop=True)\n",
    "full_grouped = (\n",
    "    full_df.groupby([\"Strategy\", \"Method\", \"Selection\"])[f\"{component}\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3716009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the lists and their average\n",
    "def plot_regret_lists(ax, regret_lists, label, avg_label):\n",
    "    # Plot individual regret lists (slightly faded)\n",
    "    for i, regret_list in enumerate(regret_lists):\n",
    "        ax.plot(np.cumsum(regret_list), alpha=0.5, label=f\"{label}\")\n",
    "    # Calculate the average regret list\n",
    "    avg_cum_regret = np.array(regret_lists).mean(axis=0).cumsum()\n",
    "    # Plot the average regret list (bold)\n",
    "    ax.plot(\n",
    "        avg_cum_regret, label=avg_label, linewidth=2.5, linestyle=\"--\", color=\"black\"\n",
    "    )\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Cumulative Regret\")\n",
    "    ax.set_title(\"Regret Over Iterations\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with dynamic number of subplots based on data\n",
    "num_rows = len(full_grouped)\n",
    "num_cols = 2  # You can change the number of columns as needed\n",
    "fig, axs = plt.subplots(\n",
    "    (num_rows + num_cols - 1) // num_cols, num_cols, figsize=(16, 6)\n",
    ")\n",
    "# Iterate through the rows in your data\n",
    "for i, (_, row) in enumerate(full_grouped.iterrows()):\n",
    "    strategy = row[\"Strategy\"]\n",
    "    method = row[\"Method\"]\n",
    "    selection = row[\"Selection\"]\n",
    "    regret_lists = row[\"Regret\"]\n",
    "    label = f\"{strategy}-{method}-{selection}\"\n",
    "    avg_label = f\"Average - {label}\"\n",
    "    # Plot in the corresponding subplot\n",
    "    plot_regret_lists(axs[i], regret_lists, label, avg_label)\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3f1f8",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BO and C-BO data\n",
    "def plot(ax, df, lambda_val, num_train):\n",
    "    bo_data = [df['Cumulative Regret'][i] for i in range(len(df['Method'])) if df['Method'][i] == 'BO']\n",
    "    cbo_data = [df['Cumulative Regret'][i] for i in range(len(df['Method'])) if df['Method'][i] == 'C-BO']\n",
    "    # Calculate mean and standard deviation\n",
    "    bo_mean = np.mean(bo_data, axis=0)\n",
    "    cbo_mean = np.mean(cbo_data, axis=0)\n",
    "    # Create x-axis values\n",
    "    x_values = np.arange(1, len(bo_mean)+1)\n",
    "    # Create a figure with a white background\n",
    "    # Plot individual BO cumulative regret lines as dashed\n",
    "    for _, bo_regret in enumerate(bo_data):\n",
    "        ax.plot(x_values, bo_regret, linestyle='--', color=\"C10\", alpha=0.2)\n",
    "    # Plot individual C-BO cumulative regret lines as dashed\n",
    "    for _, cbo_regret in enumerate(cbo_data):\n",
    "        ax.plot(x_values, cbo_regret, linestyle='--', color=\"C11\", alpha=0.2)\n",
    "    # Plot BO cumulative regrets with standard deviation fill\n",
    "    ax.plot(x_values, bo_mean, label='BO (without context)', color='C10')\n",
    "    # Plot C-BO cumulative regrets with standard deviation fill\n",
    "    ax.plot(x_values, cbo_mean, label='CBO (with context)', color='C11')\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_xticks(np.arange(5, len(bo_mean)+1, 5))\n",
    "    ax.set_ylabel('Maximum Solubility')\n",
    "    ax.set_title(rf\" $\\lambda = {lambda_val} | num_train = {num_train}$\")\n",
    "    ax.set_facecolor('lightyellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53642d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_data_collection(ax, sub_results, lambda_val, num_train):\n",
    "    data = {'Method': [], 'Strategy': [], 'Cumulative Regret': [], 'Acquisition Values': []}\n",
    "    for val in sub_results:\n",
    "        for method, strategies in val.items():\n",
    "            for strategy, data_dict in strategies.items():\n",
    "                regrets = []\n",
    "                acquisition_values = []\n",
    "                for item in data_dict['Optimal Point with MMR']:\n",
    "                    regrets.append(item['Regret'])\n",
    "                    acquisition_values.append(item['Acquisition Value'])\n",
    "                # For BO, once maximum solubility is found, we can just set the remaining values to the maximum\n",
    "                # if len(regrets) != 0:\n",
    "                #     for _ in range(20-len(regrets)):\n",
    "                #         regrets.append(regrets[-1])\n",
    "                    # Caclulate regret\n",
    "                data['Method'].append(method)\n",
    "                data['Strategy'].append(strategy)\n",
    "                data['Cumulative Regret'].append(regrets)\n",
    "                data['Acquisition Values'].append(acquisition_values)\n",
    "    df = pd.DataFrame(data)\n",
    "    # Remove rows with empty 'Regret' lists\n",
    "    df = df[df['Cumulative Regret'].apply(lambda x: bool(x))]\n",
    "    df['Mean Acquisition Value'] = df['Acquisition Values'].apply(lambda x: np.mean(x))\n",
    "    df['Std Dev Acquisition Value'] = df['Acquisition Values'].apply(lambda x: np.std(x))\n",
    "    # Create a new column with 'Mean Acquisition Value ± Standard Deviation' as strings\n",
    "    df['Acquisition Confidence Interval'] = df.apply(lambda row: f\"{row['Mean Acquisition Value']:.2f} ± {row['Std Dev Acquisition Value']:.2f}\", axis=1)\n",
    "    # Drop the separate 'Mean Acquisition Value' and 'Std Dev Acquisition Value' columns\n",
    "    df.drop(['Mean Acquisition Value', 'Std Dev Acquisition Value', \"Acquisition Values\"], axis=1, inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Calculate mean and standard deviation of acquisition values\n",
    "    df['Cumulative Regret'] = df['Cumulative Regret'].apply(lambda x: np.cumsum(x))\n",
    "    plot(ax, df, lambda_val, num_train)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16beb99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Update the None argument with the file path to the main data results\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), constrained_layout=True)\n",
    "df_4 = bo_data_collection(axs[0], sub_results=loaded_data[\"gpt-3.5-turbo/15/1\"], lambda_val=1, num_train=15)\n",
    "df_5 = bo_data_collection(axs[1], sub_results=loaded_data[\"gpt-3.5-turbo/15/5\"], lambda_val=5, num_train=15)\n",
    "df_6 = bo_data_collection(axs[2], sub_results=loaded_data[\"gpt-3.5-turbo/15/10\"], lambda_val=10, num_train=15)\n",
    "fig.patch.set_facecolor('white')\n",
    "fig.suptitle(\"Contextual Bayesian Optimisation (GPT-3.5-Turbo)\", fontsize=20, fontweight='bold', y=1.1)\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0), fancybox=True, shadow=True, ncol=2)\n",
    "plt.savefig(\"extra_ones\", dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65101cd",
   "metadata": {},
   "source": [
    "<DIV STYLE=\"background-color:#000000; height:10px; width:100%;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
